\chapter{Theory}
\label{chap:background}

In this chapter important theory that is the basis of my implementation is presented. R-trees are covered in detail, in addition to different bulk-insertion methods. LSM-trees are also described with challenges that come with its implementation. 

\section{RUM}
The RUM conjecture was introduced in 2016 by Athanassoulis et al\cite{RUM}. It looks at the challenges presented when designing new access methods and the impact it has on read- , write- and memory overhead. The main goal when designing access methods is to minimize read time, update costs and memory usage. Athanassoulis et al classifies the different overheads by these definitions: \newline

\textbf{Read Overhead}: The ratio between the total amount of data read including auxiliary (e.g. indexed) and base data, divided by the amount of retrieved data. Also known as \emph{read amplification}.\newline

\textbf{Update Overhead}: The ratio between the size of the physical updates performed for one logical update, divided by the size of the logical update. Also known as \emph{write amplification}.\newline

\textbf{Memory Overhead}: The ratio between the space utilized for auxiliary and base data, divided by the space utilized for base data. Also known as \emph{space amplification}. \newline

In order to design the best access methods there are different parameters that should be taken into account. These include the application area, workload distribution and available technology. The RUM conjecture states that only two of the overheads can be prioritized and that this will create a larger overhead for the third. Examples of access methods that prioritize read operations are B+-trees and Skiplists. These methods often implies higher space amplification as additional space is used to store the index structures. Write operations are prioritized in LSM-trees and other differential structures and focuses on being able to load larger bulks of data into storage at once. Write-optimized structures often has an impact on space utilization as there might be empty blocks and data written multiple times. In addition, read access might also suffer as it can be difficult to generate efficient index structures when loading large amounts of data at once. Examples of space-optimized access methods are bloom filters which works as an existence index. This will in turn increase write overhead as compression methods are needed to minimize the storage utilized. It can also increase read overhead as indexes can be sparse and the search is less effective. The optimal ratio for read, write and space amplification is equal to one. This is however impossible to achieve for all parameters at once. It is therefore important to find the correct application area for the data you are working with.

\section{R-trees}
R-trees was first presented in 1984 by Antonin Guttman\cite{r-tree} and was created to better handle indexing and retrieval of multi-dimensional data by their spatial locations. Previous solutions such as B+-trees are not equipped to handle data with multiple dimensions as they only support one-dimensional index structures \cite{ComparisonOfAdvancedTree}. \newline

R-trees are structured as a hierarchy with a root node which points to lower nodes until the leaf level nodes are reached. The leaf level nodes contain pointers to the location of where objects are stored. To be able to store multi-dimensional data objects R-trees use n-dimensional rectangles to index objects according to a certain space. These rectangles are known as \emph{Minimum Bounding Rectangles} (MBRs) and each root or intermediate node points to the MBRs that is contained within the MBR stored in that node. In the general R-tree, each intermediate node will contain distinct MBRs, even if they are present in several. Each non-leaf node is represented in the tree by (\emph{MBR},  \emph{p}) where \emph{MBR} is the minimum bounding rectangle which spatially contains all MBRs in the child node, and \emph{p} is a pointer to a child node. The leaf-nodes is represented by (\emph{MBR}, \emph{o}) where \emph{MBR} is the minimum bounding rectangle which spatially contains the object, and \emph{o} is the object identifier. The height of the R-tree is at most \(log_mN-1\), when containing N index records. \emph{M} is the maximum number of entries that will fit in one node, and \emph{m} is the minimum number of entries in a node given by \(m <= M/2\). An example of an R-Tree structure can be seen in Figure \ref{fig:RTree} and Figure \ref{fig:RtreeSpace}.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.4]{figures/RTree.pdf}
    \caption{R-Tree index structure}
    \label{fig:RTree}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.3]{figures/RTree_space.pdf}
    \caption{R-Tree space structure}
    \label{fig:RtreeSpace}
\end{figure}

\subsection{Searching}
Searching in R-trees are done in a similar way as in B-trees. A search is done by creating a search rectangle \emph{SR}, and checking all entries in the root node that contains MBRs that overlap with SR. A further search is done by traversing the subtree of the matching entries until all leaf node MBRs that overlap with SR are found. In worst-case scenarios this will lead to the whole tree being searched before finding a match. 

\subsection{Insertion}
Insertions in R-trees are done by inserting the new entry in a leaf node. This is done by searching the tree from the root node and finding the subtrees that overlap with the insertion entry. When the suitable node is found at the leaf level, the entry is placed if there is enough space in the node. If the node is full, which would lead to overflow, the node is split and the tree is adjusted upwards to suit the new index structure. The leaf node which has the least need for enlargement to fit the new entry is chosen.

\subsection{Deletion}
Deletion is done by first finding the leaf node that contains the entry that is to be deleted. This is done by checking which parts of the tree overlaps with the entry, and when the correct leaf node is found, the entry is removed. After the removal, the remaining leaf node is checked to see if it suffers from node underflow (i.e., contains fewer entries than $m$), and therefore needs readjusting. If this is the case, the tree will be restructured so that each leaf node meets the requirement of having a minimum of \emph{m} entries. 

\subsection{Splitting}
Splitting is a crucial part of working with R-trees as it is used when node overflow occurs, and the choice of splitting method can have a big impact on query performance. An important part of splitting is making as small rectangles as possible for the two new MBRs created by the split. This is to avoid searching the MBRs without results, which could be the case if the rectangles contain a lot of "empty" space which in turn could overlap with the search rectangle. The splitting methods \emph{Exhaustive Split}, \emph{Quadratic Split} and \emph{Linear Split} are presented by Guttman \cite{r-tree}. In his paper he finds that the linear split method is as good as the more expensive methods without affecting the search performance to an important extent. The quadratic split method is however said to have better overall performance as stated by Beckmann, Kriegel, Schneider and Seeger\cite{R*-tree}.

\subsubsection{Exhaustive Split}
The exhaustive split executes all possible splits and then chooses the split with the MBRs that cover the underlying rectangles with the least redundant space and smallest overlap. The exhaustive split is the optimal solution, however it is computationally heavy to perform and the number of possible splits is potentially very large.

\subsubsection{Quadratic Split}
The quadratic split is executed by finding the two entries that will create a new rectangle which fills the most space. These two entries are then put in two separate MBRs, before each remaining entry from the original node is put in one of the new MBRs by finding which will enlarge it the least. If one of the MBRs will contain less than \emph{m} entries if not all remaining entries are assigned to it, then the remaining points are assigned to that MBR. The cost of this method is quadratic in $M$, as each entry will have to be computed with each other entry when finding the largest space to do the first split. $M$ is the number of entries in a full node. 

\subsubsection{Linear Split}
The linear split is executed similarly as the quadratic split. However, instead of finding the space between each entry, it only finds the distance between the most extreme rectangles. Extreme rectangles are the ones that are the farthest apart on the x-axis. This distance is normalized by taking the width, which is the length between the outer sides of the extreme rectangles. In the end, the pair with the highest value is chosen and put into two new MBRs. The remaining entries are put in the new MBR that require the least enlargement. The cost of this method is linear in $M$. 

\subsection{R+-tree}
The R+-tree is a variant of the general R-tree and was introduced in 1987 by Sellis, Roussopoulos and Faloutsos \cite{R+Tree}. It was developed to avoid overlapping MBRs in intermediate nodes. In the general R-tree each intermediate node contains MBRs which completely covers the MBRs in the child nodes. This can lead to all nodes having to be searched for a specific data object, which is not optimal. To reduce the number of intermediate nodes to be searched, the R+-tree splits the MBRs in intermediate nodes which point to leaf nodes. This is done so these MBRs do not overlap, and instead the MBRs at leaf level can be stored in multiple nodes. An important thing to note however is that the improved search performance of R+-trees has a negative effect on the space utilisation. This is however minimal if search performance is an important factor in the use of the R-tree index. The R-tree structure shown in Figure \ref{fig:RTree} and Figure \ref{fig:RtreeSpace} are presented as an R+-structure in Figure \ref{fig:RplusTree} and Figure \ref{fig:RplusTreeSpace}. Notice that node $E$ is now stored in leaf nodes pointed to by both $A$ and the newly created node $O$, while node $G$ has been moved to node $O$. 

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.4]{figures/RPlusTree.pdf}
    \caption{R+-Tree index structure}
    \label{fig:RplusTree}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.3]{figures/RPlusTree_space.pdf}
    \caption{R+-Tree space structure}
    \label{fig:RplusTreeSpace}
\end{figure}

\subsection{R*-tree}
The R*-tree is an extension of the general R-tree which is attractive as it supports both point and spatial data without increasing the implementation cost too much. This structure was presented in 1990 by Beckmann, Kriegel, Schneider and Seeger\cite{R*-tree}. In this paper they discuss the different parts that are essential for retrieval performance.\newline

\textbf{The area covered by intermediate MBRs should be minimised}\newline
The "empty" space covered by bounding rectangles should be minimized to avoid additional access paths when conducting a search. \newline

\textbf{The overlap between intermediate MBRs should be minimised}\newline
When minimizing the overlap, the possible access paths during a search is also minimized. \newline

\textbf{The margin of intermediate MBRs should be minimised}\newline
The margin of a bounding rectangle is the sum of the rectangle sides length. Minimization of the margin will lead to more quadratic shaped rectangles, which in turn are easier to place in space without causing overlap. This will again make it easier for the parent MBRs to reduce their size. \newline

\textbf{Storage utilisation should be optimised}\newline
With higher storage utilisation, the height of the tree will be reduced which will lead to lower query costs. \newline


An important thing to note is that it is difficult to achieve both high storage utilisation when also minimising the area covered by each MBR and their overlap. This is because the shape of the rectangles and the number of entries in each node needs to be more relaxed. Reducing the margin will however most likely lead to better storage utilisation. \newline

In order to improve the retrieval performance, the R*-tree focuses on the insertion of new rectangles. When choosing the subtree for insertion, the structure focuses on finding the path which leads to the least need for \emph{overlap} enlargement to include the new entry. This is in contrast to the general R-tree where it decides based on the least need for area enlargement in total. If a leaf node is full the algorithm uses reinsertion if it is the first time a node overflow occurs at that particular level in the tree. Reinsertion is costly, but gives better query performance than node splitting. If a reinsertion has already been used on a level, a splitting method is applied. The splitting method consists of deciding a split axis among all dimensions and choosing the division of entries that lead to the least overlap between the resulting MBRs.

\subsection{Dynamic Hilbert R-trees}
Hilbert-curve is a space filling curve, shown to achieve the best clustering when compared to Z-order and Gray-code curve \cite{HilbertRTree}. The Hilbert R-tree is constructed to behave like an R-tree on search, and a B*-tree during splitting on insertion. This is done by using a method called \emph{2D-c}, which sorts the data rectangles based on the 2d-hilbert value of their centers. Leaf nodes contains the same as with a regular R-tree (MBR, o), while non-leaf nodes contain additional information about the \emph{Larges Hilbert Value} (LHV) as (MBR, p, LHV). LHV is the largest hilbert value found in the MBRs enclosed by the non-leaf node's MBR. With insertion, the Dynamic Hilbert R-tree uses a method like is done in B*-trees. In regular B-trees splitting a node is done when it overflows, and two nodes is created from the overflowed node. In B*-trees the splitting is delayed by trying to send entries to sibling nodes when the original node is overflowed. The number of siblings can be chosen as \emph{s}, and when all sibling nodes are full, the split will be conducted by creating \emph{s+1} nodes to handle the overflow entry. The best tradeoff for choosing siblings is shown to be 3-to-4 splitting, which entails having three siblings. This gives the best insertion speed, which decrease with the increase of siblings, while maintaining a good performance, which increase with the number of siblings. 

In the original presentation of the Dynamic Hilbert R-tree it is shown that it outperforms R*-trees on response time by up 28\%\cite{HilbertRTree}. It has however later been discovered that the Dynamic Hilbert R-tree is vulnerable to large objects performance-wise. In addition the proximity is not kept to a satisfying degree by the Hilbert curve as the space dimensionality increases, which leads to MBRs overlapping more in internal tree nodes\cite{RTreesTheoryApplications}.

\textbf{Skal dette være et annet sted? Kanskje ikke egentlig. Les over igjen sammen med resten i denne seksjonen.}

\section{Packing R-trees}
The general packing method for R-trees was introduced in 1985\cite{DirectSpatialSearch}. In this method the rectangles in the dataset are sorted according to their center's x-coordinate. The rectangles are then packed into $k$ groups of $n$ rectangles each (except possibly the last one). Next, the MBR for each group is found and a pointer is set to the page that stores that group. Finally, the MBRs are packed recursively bottom-up in the manner explained above until the root node is created. 

\subsection{Sort-Tile-Recursive}
The Sort-Tile-Recursive (STR) method is a way of bulk-loading multi-dimensional data into R-trees and was introduced in 1997 by Leutenegger et al.\cite{STR}. For data with a dimension of $d=2$ the data is first sorted by their surrounding rectangle's x-coordinates. The x-coordinate is set to be the center for each rectangle. Further, STR uses a method called \emph{tiling} which divides the space into $S = \sqrt{r/M}$ vertical slices, where $r$ is the number of objects and $M$ is the maximum number of objects that fit in a node. Each slice has a certain amount of rectangles so that $\sqrt{r/M}$ nodes can be packed. The number of leaf nodes $P$ is set to $\lceil{r/M}\rceil$ as this gives the needed number of nodes to fit all the objects. Each slice is then filled with a partition of size $S * M$ consecutive rectangles from the sorted list. It is important to notice that the last slice might contain fewer rectangles, as it is not guaranteed that the number of rectangles will fill the vertical slices fully. The last slice must however have a minimum of $m$ rectangles. Further, the rectangles for each slice are sorted internally by their y-coordinate and $M$ rectangles at a time are packed into nodes. To complete the tree $M$ consecutive nodes are grouped together and put into a parent node. The last parent node might contain fewer than $M$ elements, but must have more than $m$. This is continued in a bottom-up manner until the root node is created. The Sort-Tile-Recursive method is also applicable for dimensions larger than two. \newline

It has been shown that the STR method have performed better than other bulk-loading methods when the data is slightly skewed or have an uniform distribution. For highly skewed data, the performance gap between STR and the Hilbert packing method is quite similar\cite{STR}. The advantage with using the STR method is that in general there are less overlap between rectangles, which leads to fewer disk accesses as fewer elements of the tree needs to be searched. The difference in MBR overlap between the traditional R-tree, the improved R*-tree and the Sort-Tile-Recursive packed R-tree is shown in Figure \ref{fig:STRTree}.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.7]{figures/STR.png}
    \caption{MBRs of leaf nodes for R-tree, R*-tree and STR Packed R-tre\cite{RTreesTheoryApplications}}
    \label{fig:STRTree}
\end{figure}


\textbf{SKRIV OM HILBERT PACKING METHOD, SIDEN DEN ER NEVNT I Sort-Tile-Recursive}

\subsection{Small-Tree-Large-Tree Bulk-Insertion}
The Small-Tree-Large-Tree (STLT) bulk-insertion technique was presented in 1998 by L. Chen et al\cite{STLT} and is the first approach where bulk-insertion is used to update an existing R-tree, instead of creating an R-tree from scratch. Important aspects to conducting insertions into an existing R-tree is to make sure that the down-time is low so that queries can be conducted and that the query performance is not degraded to a large extent. In addition, it will reduce the insertion time as one does not have to build the entire R-tree from scratch with every insertion. To achieve bulk-insertion into an existing index structure, the STLT technique is designed for skewed data, i.e. data that are located in the same region. When new data is added an R-tree index structure is created from it, which is called a small tree. A suitable location in the existing R-tree, also called the large tree, is found for insertion and the small tree is inserted here. By having skewed data, it can be ensured that the data inserted belongs to a certain location in the existing R-tree. In addition, down-time is reduced as only a small portion of the large tree is locked during the insertion.\newline

The Small-Tree-Large-Tree method was also developed further in 1999 by introducing a Generalized Bulk-Insertion strategy (GBI)\cite{GBI}. GBI was introduced as the STLT technique only performs well with skewed data, while other distributions lead to overlapping MBRs which again have a degrading effect on the index structure and query performance. In the GBI approach, the data to be inserted is first clustered by using k-means. Small R-trees are then created from these clusters for a STLT insertion process. Outliers are inserted one by one. The advantage of GBI is that it is better suited for different types of data distributions, in addition to having query performance which is comparable to the traditional one-by-one approach proposed by Guttman\cite{r-tree}.

\subsection{Bulk-Insertion by Seeded Clustering}
Bulk-Insertion by Seeded Clustering was introduced by T. Lee et al in 2003 to maintain an efficient index structure for R-trees when bulk-insertion is utilized\cite{SeededClustering}. The general idea is to create input R-trees from clusters and inserting them into a single large, target R-tree. The seeded R-tree is first created from the top $k$ levels of the target R-tree, and this structure is used to classify data items into clusters. An input R-tree is then created from each of these clusters, which is shown to create less overlap between MBRs during bulk insertion. The insertion process is done by inserting the input trees into a node at height $h$ in the target R-tree. This height is equal to the height of the input R-tree, as this is required to maintain that all leaf nodes are at the same level. Another important part of the insertion is to make sure that the intermediate target node created during insertion does not suffer from node underflow, which is when the input root node contains fewer entries than $m$. To handle this issue, the entries of the root node of the input R-tree are reinserted into the target node. If the root node contains a number of entries higher than $m$, the input tree is directly inserted into the target node.\newline

Finally, a repacking method is used if the root node from the input tree and the target node have many overlapping entries. This is done to maintain a good query performance. The method is done by repacking the overlapping entries from the target node with the input node. This is done in a bottom-up manner by first repacking the children and then repeating the process upwards. Bulk-insertion by seeded clustering has shown to outperform other bulk-insertion methods such as the Generalized Bulk-Insertion (GBI) with regard to both insertion and query performance. In addition, it has proven to have better query performance than R-trees with insertion one-by-one\cite{SeededClustering}.

\section{The Log-Structured Merge-Tree}
The \emph{Log-Structured Merge-tree} was introduced in 1996 by O'Neil et al\cite{LSMTree}. It is a write-optimized storage structure created to handle big amounts of insertions, while still being able to search for entries by index. The structure of LSM-trees consists of multiple levels. The top level is in memory, known as $C_0$ where entries are stored in a tree by index. The lower levels $C_1, C_2 ... C_n$ are stored on disk and store its entries in a B-tree like index structure. Instead of inserting one entry at a time, the LSM-tree loads entries to disk in batches by using a method called \emph{rolling merge}. The in-memory component $C_0$ is smaller than the lower components, while the lower components grow in size the further down they are in the chain. When $C_0$ reaches a treshold close to its maximum size, entries are sent to merge with $C_1$ while a batch of entries from $C_1$ are put in a buffer. The block containing the old entries from $C_1$ before merge is called the \emph{emptying} block, while the new leaf nodes merged from $C_0$ is called the \emph{filling} block. The filling block is stored on a new disk page, so the old entries are stored in case recovery is needed. This means that write operations are done in-place in $C_0$, while they are done out-of-place in the lower components. The structure of LSM-trees with two levels can be seen in Figure \ref{fig:LSMTree}, while the rolling merge process can be seen in Figure \ref{fig:LSMMerge}. 

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.3]{figures/LSM.png}
    \caption{LSM-tree with two levels\cite{LSMTree}}
    \label{fig:LSMTree}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.7]{figures/merge_LSM.pdf}
    \caption{Rolling merge\cite{LSMTree}}
    \label{fig:LSMMerge}
\end{figure}

Delete operations in the Log-Structured Merge-tree is processed by first searching for the entry in $C_0$. If found it is removed, while if the entry is not present a \emph{delete node entry} is created and placed by index and noting an entry row to delete. This block is then sent through the levels by following the rolling merge method as is done with insertions, and if the entry is found it is deleted. Updates are handled by first deleting an entry and then inserting a new one.
\newline

An important part of optimising the LSM-tree is to find the most suitable sizes for the different components. The size of the in-memory component $C_1$ can be costly if it is too big, but must be big enough to decrease the amount of I/O operations needed to move entries to disk. The ratio between the sizes of $C_0$ and $C_1$ is given by $1/M$, where \emph{M} is the average number of entries merged during the rolling merge process between the two. The LSM-tree is originally designed with magnetic hard drives in mind, using sequential writes to optimize the use of the disk arm. This is however also a good solution for solid state drives (SDDs), as there is not a big gap performance-wise between sequential and random writes. In addition, the use of out-of-place updates is well suited as SSDs are prone to wear-down\cite{LSMSDD}.

\subsection{Today's LSM-trees}
The structure of LSM-trees have evolved over time, and is now widely used by different NoSQL systems such as Cassandra\cite{Cassandra} and Bigtable\cite{Bigtable}. The basic structure of the LSM-tree used today is similar to the one described in 1996 \cite{LSMTree}, but instead of merging higher levels into components in the lower levels, components are merged together into a new one instead\cite{LSMSurvey}. In addition to B+-trees, \emph{Sorted-String Tables} (SSTables) are often used as index structures. SSTables is made up of a list containing data blocks storing key-value pairs ordered by key, and an index block storing the key range for that list. 

\subsubsection{Merging policies}

The increase in number of components causes lower query performance, which is adressed by using merging methods to reduce them. \emph{Leveling} and \emph{Tiering} are two merge policies used in practice. Each policy organise disk componts into logical levels or tiers which are sized after a ratio \emph{T}. In the leveling merge policy, there is only one component per level and each component is \emph{T} times larges than the level above it. Entries from the upper level will therefore be merged multiple times with its sub-level, until the sub-level is full and then merged further down. The tiering merge policy has \emph{T} components per level. When a level is full, all of its components are merged together into a new component at the level below. The tiering merge policy is optimised for a high write-load, while the leveling merge policy is better performance-wise for query performance as the number of components to search are reduced\cite{LSMSurvey}. 

\subsubsection{Challenges}
With today's LSM-trees there are still areas that can be improved. \newline

\emph{Write amplification} is an issue as data has to be written multiple times when sending it through levels for merging. This can in the long run lead to wear-down of disk pages in SSDs. This is however improved with the use of the tiering merge policy as it has better write performance. Examples of this can be found in the WriteBuffer Tree\cite{WBTree}, LWC-Tree\cite{LWCTree1}\cite{LWCTree2} and PebblesDB\cite{PebblesDB}. They all use a partitioned tiering design with vertical grouping. There also exists other methods trying to reduce the write amplification, such as merge skipping which skips levels when merging data down through the levels\cite{SkipTree} and exploiting data skew by keeping track of hot and cold entries and only sending cold entries to disk\cite{TRIAD}. Keeping the hot entries in memory saves write operations as they can be written over directly. \newline

\emph{Merge operations} are an important part of LSM-trees, and can lead to buffer cache misses and write stalls when large merge operations are processed. Examples of improving the merge performance can be to check if a page from an SSTable overlaps with any of the pages in the SSTable it is to be merged with. If not, the resulting SSTable can contain pointers instead of having to read and write the pages during merge\cite{VTTree}. Another method is to use pipelining during the merging process, as it is CPU-based. By doing this, the CPU is utilised in a more effective manner\cite{ZhangEtAl}. To avoid buffer cache misses external servers\cite{AhmadEtAl} can be used for large merge operations or the old SSTable (before merge) can be stored in a buffer in the underlying level\cite{LSbMTree1}\cite{LSbMTree2}. This is however mainly effective on skewed workloads as storing cold pages additionally is not optimal. To minimize write stalls the write-throughput can be limited by only allowing a merge operation in a level to continue if the previous merge operation in that level has finished\cite{bLSM}. \newline

\emph{Hardware} has evolved since LSM-trees was first introduced. As LSM-trees was designed based on magnetic hard drives it is important to find ways to find better ways to utilize the new storage devices and their benefits. \newline

\emph{Special workloads} also have an effect on the structure of LSM-trees. The distribution can for example be a factor that should be taken into account when deciding the implementation. \newline

\emph{Auto-tuning} is mostly ways of optimising the LSM-tree for the intended use. As explained in the RUM conjecture\cite{RUM}, access methods cannot be read- write and space-optimal at the same time. The different ways of tuning the LSM-tree can therefore be a great way to adapt to different workloads and in which ways they are handled. A challenge with auto-tuning is that there are many different aspects to tune, such as choosing the correct merge policy and finding the correct size ratio between components.
